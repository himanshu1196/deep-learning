{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level language model - Dinosaurus Island\n",
    "\n",
    "Welcome to Dinosaurus Island! 65 million years ago, dinosaurs existed, and in this assignment they are back. You are in charge of a special task. Leading biology researchers are creating new breeds of dinosaurs and bringing them to life on earth, and your job is to give names to these dinosaurs. If a dinosaur does not like its name, it might go berserk, so choose wisely! \n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/dino.jpg\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>\n",
    "\n",
    "Luckily you have learned some deep learning and you will use it to save the day. Your assistant has collected a list of all the dinosaur names they could find, and compiled them into this [dataset](dinos.txt). (Feel free to take a look by clicking the previous link.) To create new dinosaur names, you will build a character level language model to generate new names. Your algorithm will learn the different name patterns, and randomly generate new names. Hopefully this algorithm will keep you and your team safe from the dinosaurs' wrath! \n",
    "\n",
    "By completing this assignment you will learn:\n",
    "\n",
    "- How to store text data for processing using an RNN \n",
    "- How to synthesize data, by sampling predictions at each time step and passing it to the next RNN-cell unit\n",
    "- How to build a character-level text generation recurrent neural network\n",
    "- Why clipping the gradients is important\n",
    "\n",
    "We will begin by loading in some functions that we have provided for you in `rnn_utils`. Specifically, you have access to functions such as `rnn_forward` and `rnn_backward` which are equivalent to those you've implemented in the previous assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Updates</font>\n",
    "\n",
    "#### If you were working on the notebook before this update...\n",
    "* The current notebook is version \"3a\".\n",
    "* You can find your original work saved in the notebook with the previous version name (\"v3\") \n",
    "* To view the file directory, go to the menu \"File->Open\", and this will open a new tab that shows the file directory.\n",
    "\n",
    "#### List of updates\n",
    "* Sort and print `chars` list of characters.\n",
    "* Import and use pretty print\n",
    "* `clip`: \n",
    "    - Additional details on why we need to use the \"out\" parameter.\n",
    "    - Modified for loop to have students fill in the correct items to loop through.\n",
    "    - Added a test case to check for hard-coding error.\n",
    "* `sample`\n",
    "    - additional hints added to steps 1,2,3,4.\n",
    "    - \"Using 2D arrays instead of 1D arrays\".\n",
    "    - explanation of numpy.ravel().\n",
    "    - fixed expected output.\n",
    "    - clarified comments in the code.\n",
    "* \"training the model\"\n",
    "    - Replaced the sample code with explanations for how to set the index, X and Y (for a better learning experience).\n",
    "* Spelling, grammar and wording corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import random\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Problem Statement\n",
    "\n",
    "### 1.1 - Dataset and Preprocessing\n",
    "\n",
    "Run the following cell to read the dataset of dinosaur names, create a list of unique characters (such as a-z), and compute the dataset and vocabulary size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* The characters are a-z (26 characters) plus the \"\\n\" (or newline character).\n",
    "* In this assignment, the newline character \"\\n\" plays a role similar to the `<EOS>` (or \"End of sentence\") token we had discussed in lecture.  \n",
    "    - Here, \"\\n\" indicates the end of the dinosaur name rather than the end of a sentence. \n",
    "* `char_to_ix`: In the cell below, we create a python dictionary (i.e., a hash table) to map each character to an index from 0-26.\n",
    "* `ix_to_char`: We also create a second python dictionary that maps each index back to the corresponding character. \n",
    "    -  This will help you figure out what index corresponds to what character in the probability distribution output of the softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Overview of the model\n",
    "\n",
    "Your model will have the following structure: \n",
    "\n",
    "- Initialize parameters \n",
    "- Run the optimization loop\n",
    "    - Forward propagation to compute the loss function\n",
    "    - Backward propagation to compute the gradients with respect to the loss function\n",
    "    - Clip the gradients to avoid exploding gradients\n",
    "    - Using the gradients, update your parameters with the gradient descent update rule.\n",
    "- Return the learned parameters \n",
    "    \n",
    "<img src=\"images/rnn.png\" style=\"width:450;height:300px;\">\n",
    "<caption><center> **Figure 1**: Recurrent Neural Network, similar to what you had built in the previous notebook \"Building a Recurrent Neural Network - Step by Step\".  </center></caption>\n",
    "\n",
    "* At each time-step, the RNN tries to predict what is the next character given the previous characters. \n",
    "* The dataset $\\mathbf{X} = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ is a list of characters in the training set.\n",
    "* $\\mathbf{Y} = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$ is the same list of characters but shifted one character forward. \n",
    "* At every time-step $t$, $y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$.  The prediction at time $t$ is the same as the input at time $t + 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Building blocks of the model\n",
    "\n",
    "In this part, you will build two important blocks of the overall model:\n",
    "- Gradient clipping: to avoid exploding gradients\n",
    "- Sampling: a technique used to generate characters\n",
    "\n",
    "You will then apply these two functions to build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Clipping the gradients in the optimization loop\n",
    "\n",
    "In this section you will implement the `clip` function that you will call inside of your optimization loop. \n",
    "\n",
    "#### Exploding gradients\n",
    "* When gradients are very large, they're called \"exploding gradients.\"  \n",
    "* Exploding gradients make the training process more difficult, because the updates may be so large that they \"overshoot\" the optimal values during back propagation.\n",
    "\n",
    "Recall that your overall loop structure usually consists of:\n",
    "* forward pass, \n",
    "* cost computation, \n",
    "* backward pass, \n",
    "* parameter update. \n",
    "\n",
    "Before updating the parameters, you will perform gradient clipping to make sure that your gradients are not \"exploding.\"\n",
    "\n",
    "#### gradient clipping\n",
    "In the exercise below, you will implement a function `clip` that takes in a dictionary of gradients and returns a clipped version of gradients if needed. \n",
    "* There are different ways to clip gradients.\n",
    "* We will use a simple element-wise clipping procedure, in which every element of the gradient vector is clipped to lie between some range [-N, N]. \n",
    "* For example, if the N=10\n",
    "    - The range is [-10, 10]\n",
    "    - If any component of the gradient vector is greater than 10, it is set to 10.\n",
    "    - If any component of the gradient vector is less than -10, it is set to -10. \n",
    "    - If any components are between -10 and 10, they keep their original values.\n",
    "\n",
    "<img src=\"images/clip.png\" style=\"width:400;height:150px;\">\n",
    "<caption><center> **Figure 2**: Visualization of gradient descent with and without gradient clipping, in a case where the network is running into \"exploding gradient\" problems. </center></caption>\n",
    "\n",
    "**Exercise**: \n",
    "Implement the function below to return the clipped gradients of your dictionary `gradients`. \n",
    "* Your function takes in a maximum threshold and returns the clipped versions of the gradients. \n",
    "* You can check out [numpy.clip](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.clip.html). \n",
    "    - You will need to use the argument \"`out = ...`\".\n",
    "    - Using the \"`out`\" parameter allows you to update a variable \"in-place\".\n",
    "    - If you don't use \"`out`\" argument, the clipped variable is stored in the variable \"gradient\" but does not update the gradient variables `dWax`, `dWaa`, `dWya`, `db`, `dby`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED FUNCTION: clip\n",
    "\n",
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    dWaa[dWaa > maxValue] = maxValue\n",
    "    dWaa[dWaa < -maxValue] = -maxValue\n",
    "    \n",
    "    dWax[dWax > maxValue] = maxValue\n",
    "    dWax[dWax < -maxValue] = -maxValue\n",
    "    \n",
    "    dWya[dWya > maxValue] = maxValue\n",
    "    dWya[dWya < -maxValue] = -maxValue\n",
    "    \n",
    "    db[db > maxValue] = maxValue\n",
    "    db[db < -maxValue] = -maxValue\n",
    "    \n",
    "    dby[dby > maxValue] = maxValue\n",
    "    dby[dby < -maxValue] = -maxValue\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a maxvalue of 10\n",
    "maxValue = 10\n",
    "np.random.seed(3)\n",
    "dWax = np.random.randn(5,3)*10\n",
    "dWaa = np.random.randn(5,5)*10\n",
    "dWya = np.random.randn(2,5)*10\n",
    "db = np.random.randn(5,1)*10\n",
    "dby = np.random.randn(2,1)*10\n",
    "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "gradients = clip(gradients, maxValue)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected output:**\n",
    "\n",
    "```Python\n",
    "gradients[\"dWaa\"][1][2] = 10.0\n",
    "gradients[\"dWax\"][3][1] = -10.0\n",
    "gradients[\"dWya\"][1][2] = 0.29713815361\n",
    "gradients[\"db\"][4] = [ 10.]\n",
    "gradients[\"dby\"][1] = [ 8.45833407]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dWaa\"][1][2] = 5.0\n",
      "gradients[\"dWax\"][3][1] = -5.0\n",
      "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
      "gradients[\"db\"][4] = [5.]\n",
      "gradients[\"dby\"][1] = [5.]\n"
     ]
    }
   ],
   "source": [
    "# Test with a maxValue of 5\n",
    "maxValue = 5\n",
    "np.random.seed(3)\n",
    "dWax = np.random.randn(5,3)*10\n",
    "dWaa = np.random.randn(5,5)*10\n",
    "dWya = np.random.randn(2,5)*10\n",
    "db = np.random.randn(5,1)*10\n",
    "dby = np.random.randn(2,1)*10\n",
    "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "gradients = clip(gradients, maxValue)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output: **\n",
    "```Python\n",
    "gradients[\"dWaa\"][1][2] = 5.0\n",
    "gradients[\"dWax\"][3][1] = -5.0\n",
    "gradients[\"dWya\"][1][2] = 0.29713815361\n",
    "gradients[\"db\"][4] = [ 5.]\n",
    "gradients[\"dby\"][1] = [ 5.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Sampling\n",
    "\n",
    "Now assume that your model is trained. You would like to generate new text (characters). The process of generation is explained in the picture below:\n",
    "\n",
    "<img src=\"images/dinos3.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **Figure 3**: In this picture, we assume the model is already trained. We pass in $x^{\\langle 1\\rangle} = \\vec{0}$ at the first time step, and have the network sample one character at a time. </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement the `sample` function below to sample characters. You need to carry out 4 steps:\n",
    "\n",
    "- **Step 1**: Input the \"dummy\" vector of zeros $x^{\\langle 1 \\rangle} = \\vec{0}$. \n",
    "    - This is the default input before we've generated any characters. \n",
    "    We also set $a^{\\langle 0 \\rangle} = \\vec{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 2**: Run one step of forward propagation to get $a^{\\langle 1 \\rangle}$ and $\\hat{y}^{\\langle 1 \\rangle}$. Here are the equations:\n",
    "\n",
    "hidden state:  \n",
    "$$ a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t+1 \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)\\tag{1}$$\n",
    "\n",
    "activation:\n",
    "$$ z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y \\tag{2}$$\n",
    "\n",
    "prediction:\n",
    "$$ \\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })\\tag{3}$$\n",
    "\n",
    "- Details about $\\hat{y}^{\\langle t+1 \\rangle }$:\n",
    "   - Note that $\\hat{y}^{\\langle t+1 \\rangle }$ is a (softmax) probability vector (its entries are between 0 and 1 and sum to 1). \n",
    "   - $\\hat{y}^{\\langle t+1 \\rangle}_i$ represents the probability that the character indexed by \"i\" is the next character.  \n",
    "   - We have provided a `softmax()` function that you can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Hints\n",
    "\n",
    "- $x^{\\langle 1 \\rangle}$   is `x` in the code. When creating the one-hot vector, make a numpy array of zeros, with the number of rows equal to the number of unique characters, and the number of columns equal to one.  It's a 2D and not a 1D array.\n",
    "- $a^{\\langle 0 \\rangle}$ is `a_prev` in the code.  It is a numpy array of zeros, where the number of rows is $n_{a}$, and number of columns is 1.  It is a 2D array as well.  $n_{a}$ is retrieved by getting the number of columns in $W_{aa}$ (the numbers need to match in order for the matrix multiplication $W_{aa}a^{\\langle t \\rangle}$ to work.\n",
    "- [numpy.dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)\n",
    "- [numpy.tanh](https://docs.scipy.org/doc/numpy/reference/generated/numpy.tanh.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using 2D arrays instead of 1D arrays\n",
    "* You may be wondering why we emphasize that $x^{\\langle 1 \\rangle}$ and $a^{\\langle 0 \\rangle}$ are 2D arrays and not 1D vectors.\n",
    "* For matrix multiplication in numpy, if we multiply a 2D matrix with a 1D vector, we end up with with a 1D array.\n",
    "* This becomes a problem when we add two arrays where we expected them to have the same shape.\n",
    "* When two arrays with  a different number of dimensions are added together, Python \"broadcasts\" one across the other.\n",
    "* Here is some sample code that shows the difference between using a 1D and 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix1 \n",
      " [[1 1]\n",
      " [2 2]\n",
      " [3 3]] \n",
      "\n",
      "matrix2 \n",
      " [[0]\n",
      " [0]\n",
      " [0]] \n",
      "\n",
      "vector1D \n",
      " [1 1] \n",
      "\n",
      "vector2D \n",
      " [[1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "matrix1 = np.array([[1,1],[2,2],[3,3]]) # (3,2)\n",
    "matrix2 = np.array([[0],[0],[0]]) # (3,1) \n",
    "vector1D = np.array([1,1]) # (2,) \n",
    "vector2D = np.array([[1],[1]]) # (2,1)\n",
    "print(\"matrix1 \\n\", matrix1,\"\\n\")\n",
    "print(\"matrix2 \\n\", matrix2,\"\\n\")\n",
    "print(\"vector1D \\n\", vector1D,\"\\n\")\n",
    "print(\"vector2D \\n\", vector2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiply 2D and 1D arrays: result is a 1D array\n",
      " [2 4 6]\n",
      "Multiply 2D and 2D arrays: result is a 2D array\n",
      " [[2]\n",
      " [4]\n",
      " [6]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Multiply 2D and 1D arrays: result is a 1D array\\n\", \n",
    "      np.dot(matrix1,vector1D))\n",
    "print(\"Multiply 2D and 2D arrays: result is a 2D array\\n\", \n",
    "      np.dot(matrix1,vector2D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding (3 x 1) vector to a (3 x 1) vector is a (3 x 1) vector\n",
      " This is what we want here!\n",
      " [[2]\n",
      " [4]\n",
      " [6]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding (3 x 1) vector to a (3 x 1) vector is a (3 x 1) vector\\n\",\n",
    "      \"This is what we want here!\\n\", \n",
    "      np.dot(matrix1,vector2D) + matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding a (3,) vector to a (3 x 1) vector\n",
      " broadcasts the 1D array across the second dimension\n",
      " Not what we want here!\n",
      " [[2 4 6]\n",
      " [2 4 6]\n",
      " [2 4 6]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding a (3,) vector to a (3 x 1) vector\\n\",\n",
    "      \"broadcasts the 1D array across the second dimension\\n\",\n",
    "      \"Not what we want here!\\n\",\n",
    "      np.dot(matrix1,vector1D) + matrix2\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 3**: Sampling: \n",
    "    - Now that we have $y^{\\langle t+1 \\rangle}$, we want to select the next letter in the dinosaur name. If we select the most probable, the model will always generate the same result given a starting letter. \n",
    "        - To make the results more interesting, we will use np.random.choice to select a next letter that is likely, but not always the same.\n",
    "    - Sampling is the selection of a value from a group of values, where each value has a probability of being picked.  \n",
    "    - Sampling allows us to generate random sequences of values.\n",
    "    - Pick the next character's index according to the probability distribution specified by $\\hat{y}^{\\langle t+1 \\rangle }$. \n",
    "    - This means that if $\\hat{y}^{\\langle t+1 \\rangle }_i = 0.16$, you will pick the index \"i\" with 16% probability. \n",
    "    - You can use [np.random.choice](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html).\n",
    "\n",
    "    Example of how to use `np.random.choice()`:\n",
    "    ```python\n",
    "    np.random.seed(0)\n",
    "    probs = np.array([0.1, 0.0, 0.7, 0.2])\n",
    "    idx = np.random.choice([0, 1, 2, 3] p = probs)\n",
    "    ```\n",
    "    - This means that you will pick the index (`idx`) according to the distribution: \n",
    "\n",
    "    $P(index = 0) = 0.1, P(index = 1) = 0.0, P(index = 2) = 0.7, P(index = 3) = 0.2$.\n",
    "\n",
    "    - Note that the value that's set to `p` should be set to a 1D vector.\n",
    "    - Also notice that $\\hat{y}^{\\langle t+1 \\rangle}$, which is `y` in the code, is a 2D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Additional Hints\n",
    "- [range](https://docs.python.org/3/library/functions.html#func-range)\n",
    "- [numpy.ravel](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ravel.html) takes a multi-dimensional array and returns its contents inside of a 1D vector.\n",
    "```Python\n",
    "arr = np.array([[1,2],[3,4]])\n",
    "print(\"arr\")\n",
    "print(arr)\n",
    "print(\"arr.ravel()\")\n",
    "print(arr.ravel())\n",
    "```\n",
    "Output:\n",
    "```Python\n",
    "arr\n",
    "[[1 2]\n",
    " [3 4]]\n",
    "arr.ravel()\n",
    "[1 2 3 4]\n",
    "```\n",
    "\n",
    "- Note that `append` is an \"in-place\" operation.  In other words, don't do this:\n",
    "```Python\n",
    "fun_hobbies = fun_hobbies.append('learning')  ## Doesn't give you what you want\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 4**: Update to $x^{\\langle t \\rangle }$ \n",
    "    - The last step to implement in `sample()` is to update the variable `x`, which currently stores $x^{\\langle t \\rangle }$, with the value of $x^{\\langle t + 1 \\rangle }$. \n",
    "    - You will represent $x^{\\langle t + 1 \\rangle }$ by creating a one-hot vector corresponding to the character that you have chosen as your prediction. \n",
    "    - You will then forward propagate $x^{\\langle t + 1 \\rangle }$ in Step 1 and keep repeating the process until you get a \"\\n\" character, indicating that you have reached the end of the dinosaur name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Additional Hints\n",
    "- In order to reset `x` before setting it to the new one-hot vector, you'll want to set all the values to zero.\n",
    "    - You can either create a new numpy array: [numpy.zeros](https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html)\n",
    "    - Or fill all values with a single number: [numpy.ndarray.fill](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.fill.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sample\n",
    "\n",
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "    char_to_ix -- python dictionary mapping each character to an index.\n",
    "    seed -- used for grading purposes. Do not worry about it.\n",
    "\n",
    "    Returns:\n",
    "    indices -- a list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    n_x = Wax.shape[1]\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    x_t = np.zeros((n_x, 1))\n",
    "    indices = []\n",
    "    index = -1\n",
    "    counter = 0\n",
    "    while (index != char_to_ix['\\n'] and counter < 50):\n",
    "        np.random.seed(counter + seed)\n",
    "        #forward propagation\n",
    "        a_next, p_t = rnn_step_forward(parameters, a_prev, x_t)\n",
    "        #sample using p_t\n",
    "            #normalize p_t\n",
    "#         if (np.sum(p_t) != 1):\n",
    "#             p_t = p_t/np.sum(p_t)\n",
    "        \n",
    "        index = np.random.choice(a = list(range(vocab_size)), p = np.ravel(p_t))\n",
    "        indices.append(index)\n",
    "        \n",
    "        a_prev = a_next\n",
    "        x_t = np.zeros(x_t.shape)\n",
    "        x_t[index, 0] = 1\n",
    "        counter += 1\n",
    "        seed += 1\n",
    "    \n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    \n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling:\n",
      "list of sampled indices:\n",
      " [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 17, 24, 12, 3, 1, 0]\n",
      "list of sampled characters:\n",
      " ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', 'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', 'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'g', 'k', 'q', 'x', 'l', 'c', 'a', '\\n']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "_, n_a = 20, 100\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "\n",
    "indices = sample(parameters, char_to_ix, 0)\n",
    "print(\"Sampling:\")\n",
    "print(\"list of sampled indices:\\n\", indices)\n",
    "print(\"list of sampled characters:\\n\", [ix_to_char[i] for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected output:**\n",
    "\n",
    "```Python\n",
    "Sampling:\n",
    "list of sampled indices:\n",
    " [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 17, 24, 12, 13, 24, 0]\n",
    "list of sampled characters:\n",
    " ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', 'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', 'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'g', 'k', 'q', 'x', 'l', 'm', 'x', '\\n']\n",
    "```\n",
    "\n",
    "* Please note that over time, if there are updates to the back-end of the Coursera platform (that may update the version of numpy), the actual list of sampled indices and sampled characters may change. \n",
    "* If you follow the instructions given above and get an output without errors, it's possible the routine is correct even if your output doesn't match the expected output.  Submit your assignment to the grader to verify its correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Building the language model \n",
    "\n",
    "It is time to build the character-level language model for text generation. \n",
    "\n",
    "\n",
    "### 3.1 - Gradient descent \n",
    "\n",
    "* In this section you will implement a function performing one step of stochastic gradient descent (with clipped gradients). \n",
    "* You will go through the training examples one at a time, so the optimization algorithm will be stochastic gradient descent. \n",
    "\n",
    "As a reminder, here are the steps of a common optimization loop for an RNN:\n",
    "\n",
    "- Forward propagate through the RNN to compute the loss\n",
    "- Backward propagate through time to compute the gradients of the loss with respect to the parameters\n",
    "- Clip the gradients\n",
    "- Update the parameters using gradient descent \n",
    "\n",
    "**Exercise**: Implement the optimization process (one step of stochastic gradient descent). \n",
    "\n",
    "The following functions are provided:\n",
    "\n",
    "```python\n",
    "def rnn_forward(X, Y, a_prev, parameters):\n",
    "    \"\"\" Performs the forward propagation through the RNN and computes the cross-entropy loss.\n",
    "    It returns the loss' value as well as a \"cache\" storing values to be used in backpropagation.\"\"\"\n",
    "    ....\n",
    "    return loss, cache\n",
    "    \n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    \"\"\" Performs the backward propagation through time to compute the gradients of the loss with respect\n",
    "    to the parameters. It returns also all the hidden states.\"\"\"\n",
    "    ...\n",
    "    return gradients, a\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\" Updates parameters using the Gradient Descent Update Rule.\"\"\"\n",
    "    ...\n",
    "    return parameters\n",
    "```\n",
    "\n",
    "Recall that you previously implemented the `clip` function:\n",
    "\n",
    "```Python\n",
    "def clip(gradients, maxValue)\n",
    "    \"\"\"Clips the gradients' values between minimum and maximum.\"\"\"\n",
    "    ...\n",
    "    return gradients\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameters\n",
    "\n",
    "* Note that the weights and biases inside the `parameters` dictionary are being updated by the optimization, even though `parameters` is not one of the returned values of the `optimize` function. The `parameters` dictionary is passed by reference into the function, so changes to this dictionary are making changes to the `parameters` dictionary even when accessed outside of the function.\n",
    "* Python dictionaries and lists are \"pass by reference\", which means that if you pass a dictionary into a function and modify the dictionary within the function, this changes that same dictionary (it's not a copy of the dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters, vocab_size = 27)\n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)\n",
    "    gradients = clip(gradients, maxValue = 5)\n",
    "    update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 126.50397572165345\n",
      "gradients[\"dWaa\"][1][2] = 0.19470931534725341\n",
      "np.argmax(gradients[\"dWax\"]) = 93\n",
      "gradients[\"dWya\"][1][2] = -0.007773876032004315\n",
      "gradients[\"db\"][4] = [-0.06809825]\n",
      "gradients[\"dby\"][1] = [0.01538192]\n",
      "a_last[4] = [-1.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "vocab_size, n_a = 27, 100\n",
    "a_prev = np.random.randn(n_a, 1)\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "X = [12,3,5,11,22,3]\n",
    "Y = [4,14,11,22,25, 26]\n",
    "\n",
    "loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "print(\"Loss =\", loss)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
    "print(\"a_last[4] =\", a_last[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected output:**\n",
    "\n",
    "```Python\n",
    "Loss = 126.503975722\n",
    "gradients[\"dWaa\"][1][2] = 0.194709315347\n",
    "np.argmax(gradients[\"dWax\"]) = 93\n",
    "gradients[\"dWya\"][1][2] = -0.007773876032\n",
    "gradients[\"db\"][4] = [-0.06809825]\n",
    "gradients[\"dby\"][1] = [ 0.01538192]\n",
    "a_last[4] = [-1.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given the dataset of dinosaur names, we use each line of the dataset (one name) as one training example. \n",
    "* Every 100 steps of stochastic gradient descent, you will sample 10 randomly chosen names to see how the algorithm is doing. \n",
    "* Remember to shuffle the dataset, so that stochastic gradient descent visits the examples in random order. \n",
    "\n",
    "**Exercise**: Follow the instructions and implement `model()`. When `examples[index]` contains one dinosaur name (string), to create an example (X, Y), you can use this:\n",
    "\n",
    "##### Set the index `idx` into the list of examples\n",
    "* Using the for-loop, walk through the shuffled list of dinosaur names in the list \"examples\".\n",
    "* If there are 100 examples, and the for-loop increments the index to 100 onwards, think of how you would make the index cycle back to 0, so that we can continue feeding the examples into the model when j is 100, 101, etc.\n",
    "* Hint: 101 divided by 100 is zero with a remainder of 1.\n",
    "* `%` is the modulus operator in python.\n",
    "\n",
    "##### Extract a single example from the list of examples\n",
    "* `single_example`: use the `idx` index that you set previously to get one word from the list of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert a string into a list of characters: `single_example_chars`\n",
    "* `single_example_chars`: A string is a list of characters.\n",
    "* You can use a list comprehension (recommended over for-loops) to generate a list of characters.\n",
    "```Python\n",
    "str = 'I love learning'\n",
    "list_of_chars = [c for c in str]\n",
    "print(list_of_chars)\n",
    "```\n",
    "\n",
    "```\n",
    "['I', ' ', 'l', 'o', 'v', 'e', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert list of characters to a list of integers: `single_example_ix`\n",
    "* Create a list that contains the index numbers associated with each character.\n",
    "* Use the dictionary `char_to_ix`\n",
    "* You can combine this with the list comprehension that is used to get a list of characters from a string.\n",
    "* This is a separate line of code below, to help learners clarify each step in the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the list of input characters: `X`\n",
    "* `rnn_forward` uses the `None` value as a flag to set the input vector as a zero-vector.\n",
    "* Prepend the `None` value in front of the list of input characters.\n",
    "* There is more than one way to prepend a value to a list.  One way is to add two lists together: `['a'] + ['b']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the integer representation of the newline character `ix_newline`\n",
    "* `ix_newline`: The newline character signals the end of the dinosaur name.\n",
    "    - get the integer representation of the newline character `'\\n'`.\n",
    "    - Use `char_to_ix`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set the list of labels (integer representation of the characters): `Y`\n",
    "* The goal is to train the RNN to predict the next letter in the name, so the labels are the list of characters that are one time step ahead of the characters in the input `X`.\n",
    "    - For example, `Y[0]` contains the same value as `X[1]`  \n",
    "* The RNN should predict a newline at the last letter so add ix_newline to the end of the labels. \n",
    "    - Append the integer representation of the newline character to the end of `Y`.\n",
    "    - Note that `append` is an in-place operation.\n",
    "    - It might be easier for you to add two lists together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names. \n",
    "    \n",
    "    Arguments:\n",
    "    data -- text corpus\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    dino_names -- number of dinosaur names you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text (size of the vocabulary)\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    with open(\"dinos.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    # Shuffle list of all dinosaur names\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state of your LSTM\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        idx = j % 100\n",
    "        x_chars = [c for c in examples[idx]]\n",
    "        x_ints = [char_to_ix[c] for c in x_chars]\n",
    "        x_train = [None] + x_ints\n",
    "        y = x_train[1:] + [char_to_ix['\\n']]\n",
    "\n",
    "        curr_loss, gradients, a = optimize(x_train, y, a_prev, parameters, learning_rate = 0.01)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            # The number of dinosaur names to print\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "                \n",
    "                seed += 1  # To get the same result (for grading purposes), increment the seed by one. \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell, you should observe your model outputting random-looking characters at the first iteration. After a few thousand iterations, your model should learn to generate reasonable-looking names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 23.087336\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 26.538561\n",
      "\n",
      "Onwtorandos\n",
      "Liabaeshom\n",
      "Lusiunh\n",
      "Ojaaersa\n",
      "Wushangos\n",
      "Daadosaurus\n",
      "Tosaurus\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 21.681743\n",
      "\n",
      "Nitorodaerus\n",
      "Ieiaadosaurus\n",
      "Jusilanaus\n",
      "Naeadrosaerus\n",
      "Wuruageligus\n",
      "Ceadora\n",
      "Totancos\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 17.958180\n",
      "\n",
      "Orysinis\n",
      "Kimacaropg\n",
      "Lososaurus\n",
      "Oraadria\n",
      "Wurlamangourus\n",
      "Cacanonecoratopsis\n",
      "Totheanisisaurus\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 21.846432\n",
      "\n",
      "Ngtushangus\n",
      "Kilachosaurus\n",
      "Lotpodoia\n",
      "Neaachia\n",
      "Visrhhanoptaros\n",
      "Cabcosaurus\n",
      "Totanaia\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 17.829879\n",
      "\n",
      "Matyshangoravusaurus\n",
      "Hamachusaurus\n",
      "Itrsonoiaurus\n",
      "Magadosaurus\n",
      "Visaurasaurus\n",
      "Caacosaulus\n",
      "Totakosaurus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 15.652200\n",
      "\n",
      "Ortysaurus\n",
      "Kimaakosaurus\n",
      "Lirpillorgithus\n",
      "Oraakosaurus\n",
      "Wisoconosaurus\n",
      "Caadosaurus\n",
      "Troblangotosaurus\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 17.048338\n",
      "\n",
      "Nhopugiaanoitetitaudia\n",
      "Igacaia\n",
      "Krrprosaurus\n",
      "Nebceora\n",
      "Viseoraeumv\n",
      "Cebcoreda\n",
      "Tryptisaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 22.364020\n",
      "\n",
      "Orqsoran\n",
      "Jiacaeosaugus\n",
      "Lirysaurus\n",
      "Oraakosaugus\n",
      "Urusaoinietus\n",
      "Daacosaurus\n",
      "Tutosaurus\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 23.232625\n",
      "\n",
      "Neyrthathna\n",
      "Kiadacosaianuseiamusadtacosaisisaurhinosaurus\n",
      "Lortongghiavirtoaoganosaurutimaurotauracheciausaur\n",
      "Nadachonachusaurus\n",
      "Urus\n",
      "Ca\n",
      "Totatasaauosaurus\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 22.540233\n",
      "\n",
      "Pitowkgorahazusaugus\n",
      "Man\n",
      "Murus\n",
      "Pa\n",
      "Vtsrathjsaurusacrus\n",
      "Dacaurur\n",
      "Totapthururusadria\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 21.617738\n",
      "\n",
      "Orthrlenatonterit\n",
      "Kiaaagria\n",
      "Ltrusaurus\n",
      "Oraaeria\n",
      "Viruchannitasaurus\n",
      "Ceaeropa\n",
      "Totasauritasosaurus\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 19.125072\n",
      "\n",
      "Otthosaurus\n",
      "Lenachoraiens\n",
      "Murusaurus\n",
      "Oraaeria\n",
      "Veotelapodterathratora\n",
      "Caadosaurus\n",
      "Totapauritaurus\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 20.018615\n",
      "\n",
      "Nitotoliesia\n",
      "Ilebalosaceus\n",
      "Krrosaurus\n",
      "Neaacisaurus\n",
      "Vismamanonusaus\n",
      "Caadosaurus\n",
      "Totananomusonoes\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 18.789854\n",
      "\n",
      "Oryusmendisaurus\n",
      "Kiaachmoncgisaurus\n",
      "Lotisaniamerator\n",
      "Oreacosaurus\n",
      "Verocaia\n",
      "Cabinoraatoshopgus\n",
      "Troptosaurus\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 17.536738\n",
      "\n",
      "Plotosaurus\n",
      "Lamaabita\n",
      "Murutalenapupor\n",
      "Paachovenator\n",
      "Veruchia\n",
      "Dacerhelaptos\n",
      "Totadanusoions\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 17.580471\n",
      "\n",
      "Pitoshirasuer\n",
      "Lenaakosaurus\n",
      "Murusaurus\n",
      "Pabcarlia\n",
      "Vithcbicerysaurus\n",
      "Daadosaurus\n",
      "Tothokeodon\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 16.329588\n",
      "\n",
      "Povtysaurus\n",
      "Leldalosaurus\n",
      "Murusaurus\n",
      "Piaahysaurus\n",
      "Uustascis\n",
      "Daadysaurus\n",
      "Totaidon\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = model(data, ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output**\n",
    "\n",
    "The output of your model may look different, but it will look something like this:\n",
    "\n",
    "```Python\n",
    "Iteration: 34000, Loss: 22.447230\n",
    "\n",
    "Onyxipaledisons\n",
    "Kiabaeropa\n",
    "Lussiamang\n",
    "Pacaeptabalsaurus\n",
    "Xosalong\n",
    "Eiacoteg\n",
    "Troia\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You can see that your algorithm has started to generate plausible dinosaur names towards the end of the training. At first, it was generating random characters, but towards the end you could see dinosaur names with cool endings. Feel free to run the algorithm even longer and play with hyperparameters to see if you can get even better results. Our implementation generated some really cool names like `maconucon`, `marloralus` and `macingsersaurus`. Your model hopefully also learned that dinosaur names tend to end in `saurus`, `don`, `aura`, `tor`, etc.\n",
    "\n",
    "If your model generates some non-cool names, don't blame the model entirely--not all actual dinosaur names sound cool. (For example, `dromaeosauroides` is an actual dinosaur name and is in the training set.) But this model should give you a set of candidates from which you can pick the coolest! \n",
    "\n",
    "This assignment had used a relatively small dataset, so that you could train an RNN quickly on a CPU. Training a model of the english language requires a much bigger dataset, and usually needs much more computation, and could run for many hours on GPUs. We ran our dinosaur name for quite some time, and so far our favorite name is the great, undefeatable, and fierce: Mangosaurus!\n",
    "\n",
    "<img src=\"images/mangosaurus.jpeg\" style=\"width:250;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Writing like Shakespeare\n",
    "\n",
    "The rest of this notebook is optional and is not graded, but we hope you'll do it anyway since it's quite fun and informative. \n",
    "\n",
    "A similar (but more complicated) task is to generate Shakespeare poems. Instead of learning from a dataset of Dinosaur names you can use a collection of Shakespearian poems. Using LSTM cells, you can learn longer term dependencies that span many characters in the text--e.g., where a character appearing somewhere a sequence can influence what should be a different character much much later in the sequence. These long term dependencies were less important with dinosaur names, since the names were quite short. \n",
    "\n",
    "\n",
    "<img src=\"images/shakespeare.jpg\" style=\"width:500;height:400px;\">\n",
    "<caption><center> Let's become poets! </center></caption>\n",
    "\n",
    "We have implemented a Shakespeare poem generator with Keras. Run the following cell to load the required packages and models. This may take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "Creating training set...\n",
      "number of training examples: 31412\n",
      "Vectorizing training set...\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91888\\.julia\\conda\\3\\envs\\keras-gpu\\lib\\site-packages\\keras\\engine\\saving.py:384: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from shakespeare_utils import *\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save you some time, we have already trained a model for ~1000 epochs on a collection of Shakespearian poems called [*\"The Sonnets\"*](shakespeare.txt). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model for one more epoch. When it finishes training for an epoch---this will also take a few minutes---you can run `generate_output`, which will prompt asking you for an input (`<`40 characters). The poem will start with your sentence, and our RNN-Shakespeare will complete the rest of the poem for you! For example, try \"Forsooth this maketh no sense \" (don't enter the quotation marks). Depending on whether you include the space at the end, your results might also differ--try it both ways, and try other inputs as well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "31412/31412 [==============================] - 78s 2ms/step - loss: 2.0898\n",
      "Epoch 2/500\n",
      "31412/31412 [==============================] - 81s 3ms/step - loss: 1.9582\n",
      "Epoch 3/500\n",
      "31412/31412 [==============================] - 78s 2ms/step - loss: 1.8912\n",
      "Epoch 4/500\n",
      "31412/31412 [==============================] - 79s 3ms/step - loss: 1.8492\n",
      "Epoch 5/500\n",
      "31412/31412 [==============================] - 77s 2ms/step - loss: 1.8137\n",
      "Epoch 6/500\n",
      "31412/31412 [==============================] - 78s 2ms/step - loss: 1.7819\n",
      "Epoch 7/500\n",
      "31412/31412 [==============================] - 81s 3ms/step - loss: 1.7624\n",
      "Epoch 8/500\n",
      "31412/31412 [==============================] - 81s 3ms/step - loss: 1.7380\n",
      "Epoch 9/500\n",
      "31412/31412 [==============================] - 81s 3ms/step - loss: 1.7169\n",
      "Epoch 10/500\n",
      "31412/31412 [==============================] - 79s 3ms/step - loss: 1.6954\n",
      "Epoch 11/500\n",
      "31412/31412 [==============================] - 491s 16ms/step - loss: 1.6719\n",
      "Epoch 12/500\n",
      "31412/31412 [==============================] - 49s 2ms/step - loss: 1.6532\n",
      "Epoch 13/500\n",
      "31412/31412 [==============================] - 69s 2ms/step - loss: 1.6313\n",
      "Epoch 14/500\n",
      "31412/31412 [==============================] - 79s 3ms/step - loss: 1.6184\n",
      "Epoch 15/500\n",
      "31412/31412 [==============================] - 78s 2ms/step - loss: 1.5954\n",
      "Epoch 16/500\n",
      "31412/31412 [==============================] - 80s 3ms/step - loss: 1.5779\n",
      "Epoch 17/500\n",
      "31412/31412 [==============================] - 81s 3ms/step - loss: 1.5606\n",
      "Epoch 18/500\n",
      "31412/31412 [==============================] - 82s 3ms/step - loss: 1.5434\n",
      "Epoch 19/500\n",
      "31412/31412 [==============================] - 80s 3ms/step - loss: 1.5272\n",
      "Epoch 20/500\n",
      "31412/31412 [==============================] - 565s 18ms/step - loss: 1.5181\n",
      "Epoch 21/500\n",
      "31412/31412 [==============================] - 32290s 1s/step - loss: 1.4969\n",
      "Epoch 22/500\n",
      "31412/31412 [==============================] - 35s 1ms/step - loss: 1.4822\n",
      "Epoch 23/500\n",
      "31412/31412 [==============================] - 37s 1ms/step - loss: 1.4665\n",
      "Epoch 24/500\n",
      "31412/31412 [==============================] - 28s 901us/step - loss: 1.4474\n",
      "Epoch 25/500\n",
      "31412/31412 [==============================] - 32s 1ms/step - loss: 1.4402\n",
      "Epoch 26/500\n",
      "31412/31412 [==============================] - 34s 1ms/step - loss: 1.4198\n",
      "Epoch 27/500\n",
      "31412/31412 [==============================] - 35s 1ms/step - loss: 1.4107\n",
      "Epoch 28/500\n",
      "31412/31412 [==============================] - 34s 1ms/step - loss: 1.4015\n",
      "Epoch 29/500\n",
      "31412/31412 [==============================] - 34s 1ms/step - loss: 1.4012\n",
      "Epoch 30/500\n",
      "31412/31412 [==============================] - 35s 1ms/step - loss: 1.3768\n",
      "Epoch 31/500\n",
      "31412/31412 [==============================] - 34s 1ms/step - loss: 1.3710\n",
      "Epoch 32/500\n",
      "31412/31412 [==============================] - 34s 1ms/step - loss: 1.3598\n",
      "Epoch 33/500\n",
      "31412/31412 [==============================] - 34s 1ms/step - loss: 1.3411\n",
      "Epoch 34/500\n",
      "31412/31412 [==============================] - 34s 1ms/step - loss: 1.3421\n",
      "Epoch 35/500\n",
      "31412/31412 [==============================] - 34s 1ms/step - loss: 1.3276\n",
      "Epoch 36/500\n",
      "31412/31412 [==============================] - 35s 1ms/step - loss: 1.3190\n",
      "Epoch 37/500\n",
      "31412/31412 [==============================] - 34s 1ms/step - loss: 1.3053\n",
      "Epoch 38/500\n",
      "31412/31412 [==============================] - 29s 918us/step - loss: 1.2960\n",
      "Epoch 39/500\n",
      "31412/31412 [==============================] - 30s 941us/step - loss: 1.2924\n",
      "Epoch 40/500\n",
      "31412/31412 [==============================] - 27s 857us/step - loss: 1.2923\n",
      "Epoch 41/500\n",
      "31412/31412 [==============================] - 26s 830us/step - loss: 1.2807\n",
      "Epoch 42/500\n",
      "31412/31412 [==============================] - 28s 880us/step - loss: 1.2623\n",
      "Epoch 43/500\n",
      "31412/31412 [==============================] - 26s 813us/step - loss: 1.2609\n",
      "Epoch 44/500\n",
      "31412/31412 [==============================] - 25s 810us/step - loss: 1.2556\n",
      "Epoch 45/500\n",
      "31412/31412 [==============================] - 26s 824us/step - loss: 1.2485\n",
      "Epoch 46/500\n",
      "31412/31412 [==============================] - 26s 823us/step - loss: 1.2443\n",
      "Epoch 47/500\n",
      "31412/31412 [==============================] - 26s 837us/step - loss: 1.2319\n",
      "Epoch 48/500\n",
      "31412/31412 [==============================] - 27s 869us/step - loss: 1.2352\n",
      "Epoch 49/500\n",
      "31412/31412 [==============================] - 30s 940us/step - loss: 1.2192\n",
      "Epoch 50/500\n",
      "31412/31412 [==============================] - 27s 847us/step - loss: 1.2155\n",
      "Epoch 51/500\n",
      "31412/31412 [==============================] - 27s 845us/step - loss: 1.2132\n",
      "Epoch 52/500\n",
      "31412/31412 [==============================] - 27s 848us/step - loss: 1.2044\n",
      "Epoch 53/500\n",
      "31412/31412 [==============================] - 27s 853us/step - loss: 1.1962\n",
      "Epoch 54/500\n",
      "31412/31412 [==============================] - 27s 853us/step - loss: 1.2010\n",
      "Epoch 55/500\n",
      "31412/31412 [==============================] - 27s 846us/step - loss: 1.1906\n",
      "Epoch 56/500\n",
      "31412/31412 [==============================] - 27s 851us/step - loss: 1.1857\n",
      "Epoch 57/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 1.1778\n",
      "Epoch 58/500\n",
      "31412/31412 [==============================] - 30s 959us/step - loss: 1.1678\n",
      "Epoch 59/500\n",
      "31412/31412 [==============================] - 28s 884us/step - loss: 1.1632\n",
      "Epoch 60/500\n",
      "31412/31412 [==============================] - 28s 890us/step - loss: 1.1557\n",
      "Epoch 61/500\n",
      "31412/31412 [==============================] - 28s 891us/step - loss: 1.1578\n",
      "Epoch 62/500\n",
      "31412/31412 [==============================] - 27s 875us/step - loss: 1.1506\n",
      "Epoch 63/500\n",
      "31412/31412 [==============================] - 28s 881us/step - loss: 1.1434\n",
      "Epoch 64/500\n",
      "31412/31412 [==============================] - 28s 882us/step - loss: 1.1402\n",
      "Epoch 65/500\n",
      "31412/31412 [==============================] - 27s 873us/step - loss: 1.1349\n",
      "Epoch 66/500\n",
      "31412/31412 [==============================] - 29s 938us/step - loss: 1.1332\n",
      "Epoch 67/500\n",
      "31412/31412 [==============================] - 28s 904us/step - loss: 1.1228\n",
      "Epoch 68/500\n",
      "31412/31412 [==============================] - 27s 865us/step - loss: 1.1310\n",
      "Epoch 69/500\n",
      "31412/31412 [==============================] - 27s 869us/step - loss: 1.1218\n",
      "Epoch 70/500\n",
      "31412/31412 [==============================] - 27s 873us/step - loss: 1.1112\n",
      "Epoch 71/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 1.1110\n",
      "Epoch 72/500\n",
      "31412/31412 [==============================] - 27s 855us/step - loss: 1.1105\n",
      "Epoch 73/500\n",
      "31412/31412 [==============================] - 27s 856us/step - loss: 1.1082\n",
      "Epoch 74/500\n",
      "31412/31412 [==============================] - 27s 862us/step - loss: 1.0947\n",
      "Epoch 75/500\n",
      "31412/31412 [==============================] - 27s 859us/step - loss: 1.0938\n",
      "Epoch 76/500\n",
      "31412/31412 [==============================] - 28s 907us/step - loss: 1.0908\n",
      "Epoch 77/500\n",
      "31412/31412 [==============================] - 27s 857us/step - loss: 1.0878\n",
      "Epoch 78/500\n",
      "31412/31412 [==============================] - 27s 863us/step - loss: 1.0857\n",
      "Epoch 79/500\n",
      "31412/31412 [==============================] - 31s 998us/step - loss: 1.0777\n",
      "Epoch 80/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 1.0775\n",
      "Epoch 81/500\n",
      "31412/31412 [==============================] - 27s 874us/step - loss: 1.0627\n",
      "Epoch 82/500\n",
      "31412/31412 [==============================] - 27s 867us/step - loss: 1.0774\n",
      "Epoch 83/500\n",
      "31412/31412 [==============================] - 27s 873us/step - loss: 1.0592\n",
      "Epoch 84/500\n",
      "31412/31412 [==============================] - 27s 869us/step - loss: 1.0577\n",
      "Epoch 85/500\n",
      "31412/31412 [==============================] - 27s 865us/step - loss: 1.0463\n",
      "Epoch 86/500\n",
      "31412/31412 [==============================] - 29s 915us/step - loss: 1.0552\n",
      "Epoch 87/500\n",
      "31412/31412 [==============================] - 27s 847us/step - loss: 1.0553\n",
      "Epoch 88/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 1.0535\n",
      "Epoch 89/500\n",
      "31412/31412 [==============================] - 27s 848us/step - loss: 1.0468\n",
      "Epoch 90/500\n",
      "31412/31412 [==============================] - 27s 853us/step - loss: 1.0469\n",
      "Epoch 91/500\n",
      "31412/31412 [==============================] - 30s 969us/step - loss: 1.0431\n",
      "Epoch 92/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 1.0442\n",
      "Epoch 93/500\n",
      "31412/31412 [==============================] - 26s 835us/step - loss: 1.0378\n",
      "Epoch 94/500\n",
      "31412/31412 [==============================] - 29s 913us/step - loss: 1.0242\n",
      "Epoch 95/500\n",
      "31412/31412 [==============================] - 28s 880us/step - loss: 1.0237\n",
      "Epoch 96/500\n",
      "31412/31412 [==============================] - 28s 876us/step - loss: 1.0187\n",
      "Epoch 97/500\n",
      "31412/31412 [==============================] - 28s 876us/step - loss: 1.0200\n",
      "Epoch 98/500\n",
      "31412/31412 [==============================] - 28s 897us/step - loss: 1.0251\n",
      "Epoch 99/500\n",
      "31412/31412 [==============================] - 29s 916us/step - loss: 1.0160\n",
      "Epoch 100/500\n",
      "31412/31412 [==============================] - 28s 883us/step - loss: 1.0246\n",
      "Epoch 101/500\n",
      "31412/31412 [==============================] - 27s 868us/step - loss: 1.0081\n",
      "Epoch 102/500\n",
      "31412/31412 [==============================] - 28s 885us/step - loss: 1.0211\n",
      "Epoch 103/500\n",
      "31412/31412 [==============================] - 30s 949us/step - loss: 1.0082\n",
      "Epoch 104/500\n",
      "31412/31412 [==============================] - 28s 901us/step - loss: 1.0156\n",
      "Epoch 105/500\n",
      "31412/31412 [==============================] - 28s 887us/step - loss: 1.0132\n",
      "Epoch 106/500\n",
      "31412/31412 [==============================] - 29s 908us/step - loss: 0.9951\n",
      "Epoch 107/500\n",
      "31412/31412 [==============================] - 27s 861us/step - loss: 1.0027\n",
      "Epoch 108/500\n",
      "31412/31412 [==============================] - 28s 880us/step - loss: 0.9888\n",
      "Epoch 109/500\n",
      "31412/31412 [==============================] - 28s 884us/step - loss: 0.9888\n",
      "Epoch 110/500\n",
      "31412/31412 [==============================] - 28s 882us/step - loss: 0.9802\n",
      "Epoch 111/500\n",
      "31412/31412 [==============================] - 27s 862us/step - loss: 0.9795\n",
      "Epoch 112/500\n",
      "31412/31412 [==============================] - 27s 875us/step - loss: 0.9801\n",
      "Epoch 113/500\n",
      "31412/31412 [==============================] - 27s 873us/step - loss: 0.9758\n",
      "Epoch 114/500\n",
      "31412/31412 [==============================] - 28s 876us/step - loss: 0.9757\n",
      "Epoch 115/500\n",
      "31412/31412 [==============================] - 29s 908us/step - loss: 0.9839\n",
      "Epoch 116/500\n",
      "31412/31412 [==============================] - 27s 873us/step - loss: 0.9819\n",
      "Epoch 117/500\n",
      "31412/31412 [==============================] - 28s 886us/step - loss: 0.9834\n",
      "Epoch 118/500\n",
      "31412/31412 [==============================] - 30s 964us/step - loss: 0.9690\n",
      "Epoch 119/500\n",
      "31412/31412 [==============================] - 28s 893us/step - loss: 0.9708\n",
      "Epoch 120/500\n",
      "31412/31412 [==============================] - 28s 884us/step - loss: 0.9661\n",
      "Epoch 121/500\n",
      "31412/31412 [==============================] - 28s 880us/step - loss: 0.9574\n",
      "Epoch 122/500\n",
      "31412/31412 [==============================] - 28s 897us/step - loss: 0.9579\n",
      "Epoch 123/500\n",
      "31412/31412 [==============================] - 29s 915us/step - loss: 0.9635\n",
      "Epoch 124/500\n",
      "31412/31412 [==============================] - 28s 895us/step - loss: 0.9576\n",
      "Epoch 125/500\n",
      "31412/31412 [==============================] - 28s 883us/step - loss: 0.9507\n",
      "Epoch 126/500\n",
      "31412/31412 [==============================] - 28s 880us/step - loss: 0.9539\n",
      "Epoch 127/500\n",
      "31412/31412 [==============================] - 28s 896us/step - loss: 0.9496\n",
      "Epoch 128/500\n",
      "31412/31412 [==============================] - 28s 892us/step - loss: 0.9534\n",
      "Epoch 129/500\n",
      "31412/31412 [==============================] - 29s 932us/step - loss: 0.9518\n",
      "Epoch 130/500\n",
      "31412/31412 [==============================] - 28s 901us/step - loss: 0.9340\n",
      "Epoch 131/500\n",
      "31412/31412 [==============================] - 28s 887us/step - loss: 0.9383\n",
      "Epoch 132/500\n",
      "31412/31412 [==============================] - 28s 896us/step - loss: 0.9448\n",
      "Epoch 133/500\n",
      "31412/31412 [==============================] - 26s 819us/step - loss: 0.9446\n",
      "Epoch 134/500\n",
      "31412/31412 [==============================] - 25s 788us/step - loss: 0.9345\n",
      "Epoch 135/500\n",
      "31412/31412 [==============================] - 25s 794us/step - loss: 0.9332\n",
      "Epoch 136/500\n",
      "31412/31412 [==============================] - 25s 789us/step - loss: 0.9330\n",
      "Epoch 137/500\n",
      "31412/31412 [==============================] - 25s 811us/step - loss: 0.9287\n",
      "Epoch 138/500\n",
      "31412/31412 [==============================] - 27s 873us/step - loss: 0.9360\n",
      "Epoch 139/500\n",
      "31412/31412 [==============================] - 29s 921us/step - loss: 0.9286\n",
      "Epoch 140/500\n",
      "31412/31412 [==============================] - 28s 892us/step - loss: 0.9208\n",
      "Epoch 141/500\n",
      "31412/31412 [==============================] - 28s 887us/step - loss: 0.9272\n",
      "Epoch 142/500\n",
      "31412/31412 [==============================] - 28s 901us/step - loss: 0.9274\n",
      "Epoch 143/500\n",
      "31412/31412 [==============================] - 28s 888us/step - loss: 0.9118\n",
      "Epoch 144/500\n",
      "31412/31412 [==============================] - 27s 874us/step - loss: 0.9127\n",
      "Epoch 145/500\n",
      "31412/31412 [==============================] - 28s 882us/step - loss: 0.9143\n",
      "Epoch 146/500\n",
      "31412/31412 [==============================] - 28s 877us/step - loss: 0.9060\n",
      "Epoch 147/500\n",
      "31412/31412 [==============================] - 28s 880us/step - loss: 0.9104\n",
      "Epoch 148/500\n",
      "31412/31412 [==============================] - 29s 913us/step - loss: 0.9122\n",
      "Epoch 149/500\n",
      "31412/31412 [==============================] - 35s 1ms/step - loss: 0.9174\n",
      "Epoch 150/500\n",
      "31412/31412 [==============================] - 28s 879us/step - loss: 0.9135\n",
      "Epoch 151/500\n",
      "31412/31412 [==============================] - 28s 876us/step - loss: 0.9027\n",
      "Epoch 152/500\n",
      "31412/31412 [==============================] - 27s 868us/step - loss: 0.9040\n",
      "Epoch 153/500\n",
      "31412/31412 [==============================] - 27s 871us/step - loss: 0.9043\n",
      "Epoch 154/500\n",
      "31412/31412 [==============================] - 28s 906us/step - loss: 0.9035\n",
      "Epoch 155/500\n",
      "31412/31412 [==============================] - 28s 882us/step - loss: 0.8887\n",
      "Epoch 156/500\n",
      "31412/31412 [==============================] - 27s 875us/step - loss: 0.8831\n",
      "Epoch 157/500\n",
      "31412/31412 [==============================] - 27s 871us/step - loss: 0.8950\n",
      "Epoch 158/500\n",
      "31412/31412 [==============================] - 28s 878us/step - loss: 0.8964\n",
      "Epoch 159/500\n",
      "31412/31412 [==============================] - 27s 875us/step - loss: 0.8943\n",
      "Epoch 160/500\n",
      "31412/31412 [==============================] - 28s 885us/step - loss: 0.8937\n",
      "Epoch 161/500\n",
      "31412/31412 [==============================] - 28s 881us/step - loss: 0.8880\n",
      "Epoch 162/500\n",
      "31412/31412 [==============================] - 28s 876us/step - loss: 0.8815\n",
      "Epoch 163/500\n",
      "31412/31412 [==============================] - 29s 918us/step - loss: 0.8892\n",
      "Epoch 164/500\n",
      "31412/31412 [==============================] - 28s 879us/step - loss: 0.87561s - l\n",
      "Epoch 165/500\n",
      "31412/31412 [==============================] - 27s 869us/step - loss: 0.8779\n",
      "Epoch 166/500\n",
      "31412/31412 [==============================] - 27s 870us/step - loss: 0.8780\n",
      "Epoch 167/500\n",
      "31412/31412 [==============================] - 28s 881us/step - loss: 0.8825\n",
      "Epoch 168/500\n",
      "31412/31412 [==============================] - 27s 867us/step - loss: 0.8682\n",
      "Epoch 169/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 0.87770s - loss\n",
      "Epoch 170/500\n",
      "31412/31412 [==============================] - 27s 874us/step - loss: 0.8847\n",
      "Epoch 171/500\n",
      "31412/31412 [==============================] - 28s 879us/step - loss: 0.8756\n",
      "Epoch 172/500\n",
      "31412/31412 [==============================] - 28s 879us/step - loss: 0.8756\n",
      "Epoch 173/500\n",
      "31412/31412 [==============================] - 28s 878us/step - loss: 0.8512\n",
      "Epoch 174/500\n",
      "31412/31412 [==============================] - 28s 889us/step - loss: 0.8753\n",
      "Epoch 175/500\n",
      "31412/31412 [==============================] - 27s 870us/step - loss: 0.8762\n",
      "Epoch 176/500\n",
      "31412/31412 [==============================] - 28s 882us/step - loss: 0.8612\n",
      "Epoch 177/500\n",
      "31412/31412 [==============================] - 28s 888us/step - loss: 0.8646\n",
      "Epoch 178/500\n",
      "31412/31412 [==============================] - 28s 877us/step - loss: 0.8611\n",
      "Epoch 179/500\n",
      "31412/31412 [==============================] - 28s 895us/step - loss: 0.8613\n",
      "Epoch 180/500\n",
      "31412/31412 [==============================] - 29s 918us/step - loss: 0.8532\n",
      "Epoch 181/500\n",
      "31412/31412 [==============================] - 27s 855us/step - loss: 0.8588\n",
      "Epoch 182/500\n",
      "31412/31412 [==============================] - 28s 885us/step - loss: 0.8571\n",
      "Epoch 183/500\n",
      "31412/31412 [==============================] - 31s 990us/step - loss: 0.8513\n",
      "Epoch 184/500\n",
      "31412/31412 [==============================] - 30s 969us/step - loss: 0.8584\n",
      "Epoch 185/500\n",
      "31412/31412 [==============================] - 30s 958us/step - loss: 0.8556\n",
      "Epoch 186/500\n",
      "31412/31412 [==============================] - 27s 865us/step - loss: 0.8636\n",
      "Epoch 187/500\n",
      "31412/31412 [==============================] - 27s 859us/step - loss: 0.8640\n",
      "Epoch 188/500\n",
      "31412/31412 [==============================] - 27s 852us/step - loss: 0.8560\n",
      "Epoch 189/500\n",
      "31412/31412 [==============================] - 29s 928us/step - loss: 0.8488\n",
      "Epoch 190/500\n",
      "31412/31412 [==============================] - 31s 989us/step - loss: 0.8399\n",
      "Epoch 191/500\n",
      "31412/31412 [==============================] - 30s 951us/step - loss: 0.8515\n",
      "Epoch 192/500\n",
      "31412/31412 [==============================] - 29s 937us/step - loss: 0.8397\n",
      "Epoch 193/500\n",
      "31412/31412 [==============================] - 30s 947us/step - loss: 0.8363\n",
      "Epoch 194/500\n",
      "31412/31412 [==============================] - 30s 939us/step - loss: 0.8394\n",
      "Epoch 195/500\n",
      "31412/31412 [==============================] - 30s 945us/step - loss: 0.8386\n",
      "Epoch 196/500\n",
      "31412/31412 [==============================] - 30s 955us/step - loss: 0.8397\n",
      "Epoch 197/500\n",
      "31412/31412 [==============================] - 30s 971us/step - loss: 0.8472\n",
      "Epoch 198/500\n",
      "31412/31412 [==============================] - 30s 950us/step - loss: 0.8365\n",
      "Epoch 199/500\n",
      "31412/31412 [==============================] - 30s 957us/step - loss: 0.8365\n",
      "Epoch 200/500\n",
      "31412/31412 [==============================] - 29s 927us/step - loss: 0.8342\n",
      "Epoch 201/500\n",
      "31412/31412 [==============================] - 31s 978us/step - loss: 0.8350\n",
      "Epoch 202/500\n",
      "31412/31412 [==============================] - 30s 970us/step - loss: 0.8329\n",
      "Epoch 203/500\n",
      "31412/31412 [==============================] - 31s 972us/step - loss: 0.8325\n",
      "Epoch 204/500\n",
      "31412/31412 [==============================] - 31s 972us/step - loss: 0.8251\n",
      "Epoch 205/500\n",
      "31412/31412 [==============================] - 30s 963us/step - loss: 0.8261\n",
      "Epoch 206/500\n",
      "31412/31412 [==============================] - 30s 969us/step - loss: 0.8262\n",
      "Epoch 207/500\n",
      "31412/31412 [==============================] - 31s 971us/step - loss: 0.8245\n",
      "Epoch 208/500\n",
      "31412/31412 [==============================] - 31s 979us/step - loss: 0.8315\n",
      "Epoch 209/500\n",
      "31412/31412 [==============================] - 31s 977us/step - loss: 0.8259\n",
      "Epoch 210/500\n",
      "31412/31412 [==============================] - 31s 991us/step - loss: 0.8237\n",
      "Epoch 211/500\n",
      "31412/31412 [==============================] - 30s 951us/step - loss: 0.8258\n",
      "Epoch 212/500\n",
      "31412/31412 [==============================] - 30s 959us/step - loss: 0.8316\n",
      "Epoch 213/500\n",
      "31412/31412 [==============================] - 30s 948us/step - loss: 0.8218\n",
      "Epoch 214/500\n",
      "31412/31412 [==============================] - 30s 947us/step - loss: 0.8088\n",
      "Epoch 215/500\n",
      "31412/31412 [==============================] - 30s 960us/step - loss: 0.8166\n",
      "Epoch 216/500\n",
      "31412/31412 [==============================] - 30s 954us/step - loss: 0.8178\n",
      "Epoch 217/500\n",
      "31412/31412 [==============================] - 30s 962us/step - loss: 0.8087\n",
      "Epoch 218/500\n",
      "31412/31412 [==============================] - 30s 970us/step - loss: 0.8065\n",
      "Epoch 219/500\n",
      "31412/31412 [==============================] - 30s 957us/step - loss: 0.8145\n",
      "Epoch 220/500\n",
      "31412/31412 [==============================] - 30s 962us/step - loss: 0.8049\n",
      "Epoch 221/500\n",
      "31412/31412 [==============================] - 30s 958us/step - loss: 0.8119\n",
      "Epoch 222/500\n",
      "31412/31412 [==============================] - 30s 956us/step - loss: 0.8104\n",
      "Epoch 223/500\n",
      "31412/31412 [==============================] - 30s 949us/step - loss: 0.8040\n",
      "Epoch 224/500\n",
      "31412/31412 [==============================] - 30s 960us/step - loss: 0.8005\n",
      "Epoch 225/500\n",
      "31412/31412 [==============================] - 30s 956us/step - loss: 0.8066\n",
      "Epoch 226/500\n",
      "31412/31412 [==============================] - 30s 952us/step - loss: 0.8023\n",
      "Epoch 227/500\n",
      "31412/31412 [==============================] - 30s 949us/step - loss: 0.8080\n",
      "Epoch 228/500\n",
      "31412/31412 [==============================] - 30s 944us/step - loss: 0.7951\n",
      "Epoch 229/500\n",
      "31412/31412 [==============================] - 31s 987us/step - loss: 0.8033\n",
      "Epoch 230/500\n",
      "31412/31412 [==============================] - 31s 972us/step - loss: 0.8008\n",
      "Epoch 231/500\n",
      "31412/31412 [==============================] - 30s 970us/step - loss: 0.8070\n",
      "Epoch 232/500\n",
      "31412/31412 [==============================] - 31s 976us/step - loss: 0.8083\n",
      "Epoch 233/500\n",
      "31412/31412 [==============================] - 31s 975us/step - loss: 0.7931\n",
      "Epoch 234/500\n",
      "31412/31412 [==============================] - 31s 979us/step - loss: 0.7931\n",
      "Epoch 235/500\n",
      "31412/31412 [==============================] - 31s 977us/step - loss: 0.8043\n",
      "Epoch 236/500\n",
      "31412/31412 [==============================] - 31s 977us/step - loss: 0.7964\n",
      "Epoch 237/500\n",
      "31412/31412 [==============================] - 31s 974us/step - loss: 0.7930\n",
      "Epoch 238/500\n",
      "31412/31412 [==============================] - 31s 973us/step - loss: 0.7931\n",
      "Epoch 239/500\n",
      "31412/31412 [==============================] - 31s 972us/step - loss: 0.7886\n",
      "Epoch 240/500\n",
      "31412/31412 [==============================] - 30s 969us/step - loss: 0.7893\n",
      "Epoch 241/500\n",
      "31412/31412 [==============================] - 31s 985us/step - loss: 0.7976\n",
      "Epoch 242/500\n",
      "31412/31412 [==============================] - 31s 981us/step - loss: 0.7906\n",
      "Epoch 243/500\n",
      "31412/31412 [==============================] - 30s 966us/step - loss: 0.7832\n",
      "Epoch 244/500\n",
      "31412/31412 [==============================] - 30s 967us/step - loss: 0.7868\n",
      "Epoch 245/500\n",
      "31412/31412 [==============================] - 30s 966us/step - loss: 0.7964\n",
      "Epoch 246/500\n",
      "31412/31412 [==============================] - 31s 972us/step - loss: 0.8015\n",
      "Epoch 247/500\n",
      "31412/31412 [==============================] - 30s 964us/step - loss: 0.7830\n",
      "Epoch 248/500\n",
      "31412/31412 [==============================] - 31s 976us/step - loss: 0.7692\n",
      "Epoch 249/500\n",
      "31412/31412 [==============================] - 31s 986us/step - loss: 0.7784\n",
      "Epoch 250/500\n",
      "31412/31412 [==============================] - 31s 979us/step - loss: 0.7833\n",
      "Epoch 251/500\n",
      "31412/31412 [==============================] - 31s 990us/step - loss: 0.7869\n",
      "Epoch 252/500\n",
      "31412/31412 [==============================] - 30s 969us/step - loss: 0.7828\n",
      "Epoch 253/500\n",
      "31412/31412 [==============================] - 31s 983us/step - loss: 0.7685\n",
      "Epoch 254/500\n",
      "31412/31412 [==============================] - 33s 1ms/step - loss: 0.7708\n",
      "Epoch 255/500\n",
      "31412/31412 [==============================] - 30s 946us/step - loss: 0.7748\n",
      "Epoch 256/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 0.7737\n",
      "Epoch 257/500\n",
      "31412/31412 [==============================] - 27s 849us/step - loss: 0.7750\n",
      "Epoch 258/500\n",
      "31412/31412 [==============================] - 27s 851us/step - loss: 0.7719\n",
      "Epoch 259/500\n",
      "31412/31412 [==============================] - 27s 867us/step - loss: 0.7690\n",
      "Epoch 260/500\n",
      "31412/31412 [==============================] - 27s 856us/step - loss: 0.7829\n",
      "Epoch 261/500\n",
      "31412/31412 [==============================] - 27s 861us/step - loss: 0.7783\n",
      "Epoch 262/500\n",
      "31412/31412 [==============================] - 27s 851us/step - loss: 0.7649\n",
      "Epoch 263/500\n",
      "31412/31412 [==============================] - 25s 798us/step - loss: 0.7767\n",
      "Epoch 264/500\n",
      "31412/31412 [==============================] - 28s 886us/step - loss: 0.7647\n",
      "Epoch 265/500\n",
      "31412/31412 [==============================] - 27s 866us/step - loss: 0.7680\n",
      "Epoch 266/500\n",
      "31412/31412 [==============================] - 27s 859us/step - loss: 0.7706\n",
      "Epoch 267/500\n",
      "31412/31412 [==============================] - 27s 847us/step - loss: 0.7565\n",
      "Epoch 268/500\n",
      "31412/31412 [==============================] - 27s 863us/step - loss: 0.7605\n",
      "Epoch 269/500\n",
      "31412/31412 [==============================] - 27s 856us/step - loss: 0.7521\n",
      "Epoch 270/500\n",
      "31412/31412 [==============================] - 27s 874us/step - loss: 0.7626\n",
      "Epoch 271/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 0.7639\n",
      "Epoch 272/500\n",
      "31412/31412 [==============================] - 28s 887us/step - loss: 0.7534\n",
      "Epoch 273/500\n",
      "31412/31412 [==============================] - 27s 864us/step - loss: 0.7547\n",
      "Epoch 274/500\n",
      "31412/31412 [==============================] - 27s 854us/step - loss: 0.7535\n",
      "Epoch 275/500\n",
      "31412/31412 [==============================] - 27s 870us/step - loss: 0.7568\n",
      "Epoch 276/500\n",
      "31412/31412 [==============================] - 27s 872us/step - loss: 0.7565\n",
      "Epoch 277/500\n",
      "31412/31412 [==============================] - 27s 862us/step - loss: 0.7646\n",
      "Epoch 278/500\n",
      "31412/31412 [==============================] - 27s 861us/step - loss: 0.7560\n",
      "Epoch 279/500\n",
      "31412/31412 [==============================] - 27s 857us/step - loss: 0.7585\n",
      "Epoch 280/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 0.7607\n",
      "Epoch 281/500\n",
      "31412/31412 [==============================] - 27s 847us/step - loss: 0.7551\n",
      "Epoch 282/500\n",
      "31412/31412 [==============================] - 27s 852us/step - loss: 0.7515\n",
      "Epoch 283/500\n",
      "31412/31412 [==============================] - 27s 860us/step - loss: 0.7644\n",
      "Epoch 284/500\n",
      "31412/31412 [==============================] - 27s 859us/step - loss: 0.7456\n",
      "Epoch 285/500\n",
      "31412/31412 [==============================] - 27s 866us/step - loss: 0.7439\n",
      "Epoch 286/500\n",
      "31412/31412 [==============================] - 28s 897us/step - loss: 0.7498\n",
      "Epoch 287/500\n",
      "31412/31412 [==============================] - 29s 913us/step - loss: 0.7532\n",
      "Epoch 288/500\n",
      "31412/31412 [==============================] - 27s 874us/step - loss: 0.7566\n",
      "Epoch 289/500\n",
      "31412/31412 [==============================] - 27s 860us/step - loss: 0.7527\n",
      "Epoch 290/500\n",
      "31412/31412 [==============================] - 27s 861us/step - loss: 0.7498\n",
      "Epoch 291/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 0.7495\n",
      "Epoch 292/500\n",
      "31412/31412 [==============================] - 28s 877us/step - loss: 0.7383\n",
      "Epoch 293/500\n",
      "31412/31412 [==============================] - 27s 863us/step - loss: 0.7491\n",
      "Epoch 294/500\n",
      "31412/31412 [==============================] - 27s 865us/step - loss: 0.7458\n",
      "Epoch 295/500\n",
      "31412/31412 [==============================] - 27s 861us/step - loss: 0.7371\n",
      "Epoch 296/500\n",
      "31412/31412 [==============================] - 27s 861us/step - loss: 0.7441\n",
      "Epoch 297/500\n",
      "31412/31412 [==============================] - 27s 868us/step - loss: 0.7469\n",
      "Epoch 298/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 0.7481\n",
      "Epoch 299/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 0.7389\n",
      "Epoch 300/500\n",
      "31412/31412 [==============================] - 27s 856us/step - loss: 0.7326\n",
      "Epoch 301/500\n",
      "31412/31412 [==============================] - 27s 870us/step - loss: 0.7392\n",
      "Epoch 302/500\n",
      "31412/31412 [==============================] - 27s 866us/step - loss: 0.7237\n",
      "Epoch 303/500\n",
      "31412/31412 [==============================] - 27s 870us/step - loss: 0.7301\n",
      "Epoch 304/500\n",
      "31412/31412 [==============================] - 27s 868us/step - loss: 0.7335\n",
      "Epoch 305/500\n",
      "31412/31412 [==============================] - 27s 851us/step - loss: 0.7451\n",
      "Epoch 306/500\n",
      "31412/31412 [==============================] - 27s 873us/step - loss: 0.7362\n",
      "Epoch 307/500\n",
      "31412/31412 [==============================] - 27s 872us/step - loss: 0.7372\n",
      "Epoch 308/500\n",
      "31412/31412 [==============================] - 27s 860us/step - loss: 0.7224\n",
      "Epoch 309/500\n",
      "31412/31412 [==============================] - 28s 881us/step - loss: 0.7236\n",
      "Epoch 310/500\n",
      "31412/31412 [==============================] - 27s 859us/step - loss: 0.7300\n",
      "Epoch 311/500\n",
      "31412/31412 [==============================] - 27s 863us/step - loss: 0.7251\n",
      "Epoch 312/500\n",
      "31412/31412 [==============================] - 27s 867us/step - loss: 0.7402\n",
      "Epoch 313/500\n",
      "31412/31412 [==============================] - 27s 867us/step - loss: 0.7249\n",
      "Epoch 314/500\n",
      "31412/31412 [==============================] - 28s 883us/step - loss: 0.7269\n",
      "Epoch 315/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 0.7296\n",
      "Epoch 316/500\n",
      "31412/31412 [==============================] - 27s 871us/step - loss: 0.7229\n",
      "Epoch 317/500\n",
      "31412/31412 [==============================] - 28s 885us/step - loss: 0.7164\n",
      "Epoch 318/500\n",
      "31412/31412 [==============================] - 27s 857us/step - loss: 0.7248\n",
      "Epoch 319/500\n",
      "31412/31412 [==============================] - 27s 863us/step - loss: 0.7264\n",
      "Epoch 320/500\n",
      "31412/31412 [==============================] - 27s 861us/step - loss: 0.7124\n",
      "Epoch 321/500\n",
      "31412/31412 [==============================] - 27s 855us/step - loss: 0.7376\n",
      "Epoch 322/500\n",
      "31412/31412 [==============================] - 27s 857us/step - loss: 0.7262\n",
      "Epoch 323/500\n",
      "31412/31412 [==============================] - 27s 864us/step - loss: 0.7201\n",
      "Epoch 324/500\n",
      "31412/31412 [==============================] - 27s 864us/step - loss: 0.7217\n",
      "Epoch 325/500\n",
      "31412/31412 [==============================] - 27s 869us/step - loss: 0.7203\n",
      "Epoch 326/500\n",
      "31412/31412 [==============================] - 27s 873us/step - loss: 0.7255\n",
      "Epoch 327/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 0.7258\n",
      "Epoch 328/500\n",
      "31412/31412 [==============================] - 27s 865us/step - loss: 0.7255\n",
      "Epoch 329/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 0.7204\n",
      "Epoch 330/500\n",
      "31412/31412 [==============================] - 27s 852us/step - loss: 0.7145\n",
      "Epoch 331/500\n",
      "31412/31412 [==============================] - 27s 860us/step - loss: 0.7237\n",
      "Epoch 332/500\n",
      "31412/31412 [==============================] - 27s 853us/step - loss: 0.7125\n",
      "Epoch 333/500\n",
      "31412/31412 [==============================] - 27s 859us/step - loss: 0.7214\n",
      "Epoch 334/500\n",
      "31412/31412 [==============================] - 27s 865us/step - loss: 0.7097\n",
      "Epoch 335/500\n",
      "31412/31412 [==============================] - 27s 861us/step - loss: 0.7196\n",
      "Epoch 336/500\n",
      "31412/31412 [==============================] - 27s 870us/step - loss: 0.7068\n",
      "Epoch 337/500\n",
      "31412/31412 [==============================] - 27s 858us/step - loss: 0.7067\n",
      "Epoch 338/500\n",
      "31412/31412 [==============================] - 27s 870us/step - loss: 0.7160\n",
      "Epoch 339/500\n",
      "31412/31412 [==============================] - 27s 862us/step - loss: 0.7120\n",
      "Epoch 340/500\n",
      "31412/31412 [==============================] - 27s 867us/step - loss: 0.7073\n",
      "Epoch 341/500\n",
      "31412/31412 [==============================] - 27s 868us/step - loss: 0.7126\n",
      "Epoch 342/500\n",
      "31412/31412 [==============================] - 27s 860us/step - loss: 0.7096\n",
      "Epoch 343/500\n",
      "31412/31412 [==============================] - 28s 891us/step - loss: 0.7041\n",
      "Epoch 344/500\n",
      "31412/31412 [==============================] - 28s 896us/step - loss: 0.6991\n",
      "Epoch 345/500\n",
      "31412/31412 [==============================] - 30s 941us/step - loss: 0.7028\n",
      "Epoch 346/500\n",
      "31412/31412 [==============================] - 27s 857us/step - loss: 0.7030\n",
      "Epoch 347/500\n",
      "31412/31412 [==============================] - 27s 845us/step - loss: 0.7138\n",
      "Epoch 348/500\n",
      "31412/31412 [==============================] - 27s 853us/step - loss: 0.7080\n",
      "Epoch 349/500\n",
      "31412/31412 [==============================] - 27s 857us/step - loss: 0.7043\n",
      "Epoch 350/500\n",
      "31412/31412 [==============================] - 27s 850us/step - loss: 0.7046\n",
      "Epoch 351/500\n",
      "31412/31412 [==============================] - 29s 919us/step - loss: 0.7039\n",
      "Epoch 352/500\n",
      "31412/31412 [==============================] - 27s 848us/step - loss: 0.7062\n",
      "Epoch 353/500\n",
      "31412/31412 [==============================] - 26s 841us/step - loss: 0.7047\n",
      "Epoch 354/500\n",
      "31412/31412 [==============================] - 27s 848us/step - loss: 0.6974\n",
      "Epoch 355/500\n",
      "31412/31412 [==============================] - 28s 887us/step - loss: 0.7019\n",
      "Epoch 356/500\n",
      "31412/31412 [==============================] - 27s 854us/step - loss: 0.7002\n",
      "Epoch 357/500\n",
      "31412/31412 [==============================] - 27s 849us/step - loss: 0.7041\n",
      "Epoch 358/500\n",
      "31412/31412 [==============================] - 27s 850us/step - loss: 0.6897\n",
      "Epoch 359/500\n",
      "31412/31412 [==============================] - 27s 847us/step - loss: 0.6945\n",
      "Epoch 360/500\n",
      "31412/31412 [==============================] - 26s 840us/step - loss: 0.7145\n",
      "Epoch 361/500\n",
      "31412/31412 [==============================] - 26s 836us/step - loss: 0.7014\n",
      "Epoch 362/500\n",
      "31412/31412 [==============================] - 27s 845us/step - loss: 0.7029\n",
      "Epoch 363/500\n",
      "31412/31412 [==============================] - 26s 835us/step - loss: 0.6930\n",
      "Epoch 364/500\n",
      "31412/31412 [==============================] - 27s 847us/step - loss: 0.6886\n",
      "Epoch 365/500\n",
      "31412/31412 [==============================] - 27s 845us/step - loss: 0.6884\n",
      "Epoch 366/500\n",
      "31412/31412 [==============================] - 26s 837us/step - loss: 0.6906\n",
      "Epoch 367/500\n",
      "31412/31412 [==============================] - 27s 845us/step - loss: 0.6911\n",
      "Epoch 368/500\n",
      "31412/31412 [==============================] - 26s 842us/step - loss: 0.69670s - loss: \n",
      "Epoch 369/500\n",
      "31412/31412 [==============================] - 26s 839us/step - loss: 0.6933\n",
      "Epoch 370/500\n",
      "31412/31412 [==============================] - 27s 844us/step - loss: 0.6938\n",
      "Epoch 371/500\n",
      "31412/31412 [==============================] - 26s 841us/step - loss: 0.6976\n",
      "Epoch 372/500\n",
      "31412/31412 [==============================] - 26s 840us/step - loss: 0.6842\n",
      "Epoch 373/500\n",
      "31412/31412 [==============================] - 27s 844us/step - loss: 0.6916\n",
      "Epoch 374/500\n",
      "31412/31412 [==============================] - 26s 844us/step - loss: 0.6944\n",
      "Epoch 375/500\n",
      "31412/31412 [==============================] - 26s 838us/step - loss: 0.69741s \n",
      "Epoch 376/500\n",
      "31412/31412 [==============================] - 26s 836us/step - loss: 0.6888\n",
      "Epoch 377/500\n",
      "31412/31412 [==============================] - 26s 837us/step - loss: 0.6835\n",
      "Epoch 378/500\n",
      "31412/31412 [==============================] - 27s 847us/step - loss: 0.6954\n",
      "Epoch 379/500\n",
      "31412/31412 [==============================] - 27s 848us/step - loss: 0.6829\n",
      "Epoch 380/500\n",
      "31412/31412 [==============================] - 26s 840us/step - loss: 0.6952\n",
      "Epoch 381/500\n",
      "31412/31412 [==============================] - 26s 837us/step - loss: 0.6901\n",
      "Epoch 382/500\n",
      "31412/31412 [==============================] - 27s 848us/step - loss: 0.6963\n",
      "Epoch 383/500\n",
      "31412/31412 [==============================] - 27s 846us/step - loss: 0.6884\n",
      "Epoch 384/500\n",
      "31412/31412 [==============================] - 27s 845us/step - loss: 0.6863\n",
      "Epoch 385/500\n",
      "31412/31412 [==============================] - 26s 841us/step - loss: 0.67631s -\n",
      "Epoch 386/500\n",
      "31412/31412 [==============================] - 26s 832us/step - loss: 0.6807\n",
      "Epoch 387/500\n",
      "31412/31412 [==============================] - 26s 834us/step - loss: 0.6834\n",
      "Epoch 388/500\n",
      "31412/31412 [==============================] - 26s 834us/step - loss: 0.6793\n",
      "Epoch 389/500\n",
      "31412/31412 [==============================] - 26s 832us/step - loss: 0.6766\n",
      "Epoch 390/500\n",
      "31412/31412 [==============================] - 26s 832us/step - loss: 0.6761\n",
      "Epoch 391/500\n",
      "31412/31412 [==============================] - 26s 839us/step - loss: 0.6782\n",
      "Epoch 392/500\n",
      "31412/31412 [==============================] - 27s 844us/step - loss: 0.6743\n",
      "Epoch 393/500\n",
      "31412/31412 [==============================] - 26s 832us/step - loss: 0.6793\n",
      "Epoch 394/500\n",
      "31412/31412 [==============================] - 26s 835us/step - loss: 0.6818\n",
      "Epoch 395/500\n",
      "31412/31412 [==============================] - 26s 838us/step - loss: 0.6672\n",
      "Epoch 396/500\n",
      "31412/31412 [==============================] - 26s 835us/step - loss: 0.6836\n",
      "Epoch 397/500\n",
      "31412/31412 [==============================] - 26s 841us/step - loss: 0.6798\n",
      "Epoch 398/500\n",
      "31412/31412 [==============================] - 26s 838us/step - loss: 0.6826\n",
      "Epoch 399/500\n",
      "31412/31412 [==============================] - 26s 842us/step - loss: 0.6709\n",
      "Epoch 400/500\n",
      "31412/31412 [==============================] - 26s 832us/step - loss: 0.6713\n",
      "Epoch 401/500\n",
      "31412/31412 [==============================] - 26s 813us/step - loss: 0.6734\n",
      "Epoch 402/500\n",
      "31412/31412 [==============================] - 25s 797us/step - loss: 0.6742\n",
      "Epoch 403/500\n",
      "31412/31412 [==============================] - 25s 802us/step - loss: 0.6768\n",
      "Epoch 404/500\n",
      "31412/31412 [==============================] - 25s 804us/step - loss: 0.6732\n",
      "Epoch 405/500\n",
      "31412/31412 [==============================] - 25s 798us/step - loss: 0.6740\n",
      "Epoch 406/500\n",
      "31412/31412 [==============================] - 26s 813us/step - loss: 0.6743\n",
      "Epoch 407/500\n",
      "31412/31412 [==============================] - 25s 792us/step - loss: 0.67140s - loss: 0.67\n",
      "Epoch 408/500\n",
      "31412/31412 [==============================] - 25s 792us/step - loss: 0.6754\n",
      "Epoch 409/500\n",
      "31412/31412 [==============================] - 25s 790us/step - loss: 0.6840\n",
      "Epoch 410/500\n",
      "31412/31412 [==============================] - 25s 794us/step - loss: 0.6713\n",
      "Epoch 411/500\n",
      "31412/31412 [==============================] - 25s 785us/step - loss: 0.6710\n",
      "Epoch 412/500\n",
      "31412/31412 [==============================] - 25s 792us/step - loss: 0.6688\n",
      "Epoch 413/500\n",
      "31412/31412 [==============================] - 25s 784us/step - loss: 0.6677\n",
      "Epoch 414/500\n",
      "31412/31412 [==============================] - 26s 820us/step - loss: 0.6634\n",
      "Epoch 415/500\n",
      "31412/31412 [==============================] - 26s 819us/step - loss: 0.6685\n",
      "Epoch 416/500\n",
      "31412/31412 [==============================] - 26s 821us/step - loss: 0.6545\n",
      "Epoch 417/500\n",
      "31412/31412 [==============================] - 26s 821us/step - loss: 0.6704\n",
      "Epoch 418/500\n",
      "31412/31412 [==============================] - 26s 826us/step - loss: 0.6774\n",
      "Epoch 419/500\n",
      "31412/31412 [==============================] - 26s 824us/step - loss: 0.6594\n",
      "Epoch 420/500\n",
      "31412/31412 [==============================] - 26s 815us/step - loss: 0.6682\n",
      "Epoch 421/500\n",
      "31412/31412 [==============================] - 26s 829us/step - loss: 0.6600\n",
      "Epoch 422/500\n",
      "31412/31412 [==============================] - 26s 818us/step - loss: 0.6696\n",
      "Epoch 423/500\n",
      "31412/31412 [==============================] - 26s 839us/step - loss: 0.6662\n",
      "Epoch 424/500\n",
      "31412/31412 [==============================] - 26s 834us/step - loss: 0.6556\n",
      "Epoch 425/500\n",
      "31412/31412 [==============================] - 26s 821us/step - loss: 0.6537\n",
      "Epoch 426/500\n",
      "31412/31412 [==============================] - 26s 832us/step - loss: 0.6517\n",
      "Epoch 427/500\n",
      "31412/31412 [==============================] - 26s 831us/step - loss: 0.6674\n",
      "Epoch 428/500\n",
      "31412/31412 [==============================] - 26s 823us/step - loss: 0.6671\n",
      "Epoch 429/500\n",
      "31412/31412 [==============================] - 26s 837us/step - loss: 0.6588\n",
      "Epoch 430/500\n",
      "31412/31412 [==============================] - 26s 833us/step - loss: 0.6593\n",
      "Epoch 431/500\n",
      "31412/31412 [==============================] - 26s 830us/step - loss: 0.6637\n",
      "Epoch 432/500\n",
      "31412/31412 [==============================] - 26s 833us/step - loss: 0.6590\n",
      "Epoch 433/500\n",
      "31412/31412 [==============================] - 26s 833us/step - loss: 0.6636\n",
      "Epoch 434/500\n",
      "31412/31412 [==============================] - 26s 829us/step - loss: 0.6582\n",
      "Epoch 435/500\n",
      "31412/31412 [==============================] - 26s 824us/step - loss: 0.6556\n",
      "Epoch 436/500\n",
      "31412/31412 [==============================] - 26s 820us/step - loss: 0.6470\n",
      "Epoch 437/500\n",
      "31412/31412 [==============================] - 26s 829us/step - loss: 0.6448\n",
      "Epoch 438/500\n",
      "31412/31412 [==============================] - 26s 822us/step - loss: 0.6591\n",
      "Epoch 439/500\n",
      "31412/31412 [==============================] - 26s 829us/step - loss: 0.6544\n",
      "Epoch 440/500\n",
      "31412/31412 [==============================] - 26s 826us/step - loss: 0.6494\n",
      "Epoch 441/500\n",
      "31412/31412 [==============================] - 26s 830us/step - loss: 0.6518\n",
      "Epoch 442/500\n",
      "31412/31412 [==============================] - 26s 833us/step - loss: 0.6528\n",
      "Epoch 443/500\n",
      "31412/31412 [==============================] - 26s 838us/step - loss: 0.6604\n",
      "Epoch 444/500\n",
      "31412/31412 [==============================] - 26s 826us/step - loss: 0.6422\n",
      "Epoch 445/500\n",
      "31412/31412 [==============================] - 26s 825us/step - loss: 0.6444\n",
      "Epoch 446/500\n",
      "31412/31412 [==============================] - 26s 831us/step - loss: 0.6487\n",
      "Epoch 447/500\n",
      "31412/31412 [==============================] - 26s 826us/step - loss: 0.6462\n",
      "Epoch 448/500\n",
      "31412/31412 [==============================] - 26s 825us/step - loss: 0.6544\n",
      "Epoch 449/500\n",
      "31412/31412 [==============================] - 26s 823us/step - loss: 0.65351\n",
      "Epoch 450/500\n",
      "31412/31412 [==============================] - 26s 829us/step - loss: 0.6348\n",
      "Epoch 451/500\n",
      "31412/31412 [==============================] - 26s 823us/step - loss: 0.6576\n",
      "Epoch 452/500\n",
      "31412/31412 [==============================] - 26s 830us/step - loss: 0.6421\n",
      "Epoch 453/500\n",
      "31412/31412 [==============================] - 26s 825us/step - loss: 0.6358\n",
      "Epoch 454/500\n",
      "31412/31412 [==============================] - 26s 837us/step - loss: 0.6532\n",
      "Epoch 455/500\n",
      "31412/31412 [==============================] - 26s 828us/step - loss: 0.6429\n",
      "Epoch 456/500\n",
      "31412/31412 [==============================] - 26s 826us/step - loss: 0.6527\n",
      "Epoch 457/500\n",
      "31412/31412 [==============================] - 27s 848us/step - loss: 0.6504\n",
      "Epoch 458/500\n",
      "31412/31412 [==============================] - 26s 829us/step - loss: 0.6369\n",
      "Epoch 459/500\n",
      "31412/31412 [==============================] - 26s 830us/step - loss: 0.6440\n",
      "Epoch 460/500\n",
      "31412/31412 [==============================] - 26s 826us/step - loss: 0.6383\n",
      "Epoch 461/500\n",
      "31412/31412 [==============================] - 26s 834us/step - loss: 0.6557\n",
      "Epoch 462/500\n",
      "31412/31412 [==============================] - 26s 835us/step - loss: 0.6560\n",
      "Epoch 463/500\n",
      "31412/31412 [==============================] - 26s 828us/step - loss: 0.6443\n",
      "Epoch 464/500\n",
      "31412/31412 [==============================] - 26s 837us/step - loss: 0.6504\n",
      "Epoch 465/500\n",
      "31412/31412 [==============================] - 26s 833us/step - loss: 0.6413\n",
      "Epoch 466/500\n",
      "31412/31412 [==============================] - 26s 837us/step - loss: 0.6362\n",
      "Epoch 467/500\n",
      "31412/31412 [==============================] - 26s 831us/step - loss: 0.6461\n",
      "Epoch 468/500\n",
      "31412/31412 [==============================] - 26s 828us/step - loss: 0.6596\n",
      "Epoch 469/500\n",
      "31412/31412 [==============================] - 26s 826us/step - loss: 0.6407\n",
      "Epoch 470/500\n",
      "31412/31412 [==============================] - 26s 831us/step - loss: 0.6483\n",
      "Epoch 471/500\n",
      "31412/31412 [==============================] - 26s 829us/step - loss: 0.6323\n",
      "Epoch 472/500\n",
      "31412/31412 [==============================] - 26s 832us/step - loss: 0.6347\n",
      "Epoch 473/500\n",
      "31412/31412 [==============================] - 26s 833us/step - loss: 0.6439\n",
      "Epoch 474/500\n",
      "31412/31412 [==============================] - 26s 836us/step - loss: 0.6397\n",
      "Epoch 475/500\n",
      "31412/31412 [==============================] - 26s 828us/step - loss: 0.6397\n",
      "Epoch 476/500\n",
      "31412/31412 [==============================] - 26s 823us/step - loss: 0.6353\n",
      "Epoch 477/500\n",
      "31412/31412 [==============================] - 26s 841us/step - loss: 0.6459\n",
      "Epoch 478/500\n",
      "31412/31412 [==============================] - 26s 827us/step - loss: 0.6295\n",
      "Epoch 479/500\n",
      "31412/31412 [==============================] - 26s 824us/step - loss: 0.6363\n",
      "Epoch 480/500\n",
      "31412/31412 [==============================] - 25s 794us/step - loss: 0.6468\n",
      "Epoch 481/500\n",
      "31412/31412 [==============================] - 25s 794us/step - loss: 0.6281\n",
      "Epoch 482/500\n",
      "31412/31412 [==============================] - 25s 789us/step - loss: 0.6312\n",
      "Epoch 483/500\n",
      "31412/31412 [==============================] - 25s 797us/step - loss: 0.6313\n",
      "Epoch 484/500\n",
      "31412/31412 [==============================] - 25s 795us/step - loss: 0.6313\n",
      "Epoch 485/500\n",
      "31412/31412 [==============================] - 25s 797us/step - loss: 0.6424\n",
      "Epoch 486/500\n",
      "31412/31412 [==============================] - 25s 796us/step - loss: 0.6367\n",
      "Epoch 487/500\n",
      "31412/31412 [==============================] - 25s 792us/step - loss: 0.6335\n",
      "Epoch 488/500\n",
      "31412/31412 [==============================] - 25s 790us/step - loss: 0.6309\n",
      "Epoch 489/500\n",
      "31412/31412 [==============================] - 25s 794us/step - loss: 0.6325\n",
      "Epoch 490/500\n",
      "31412/31412 [==============================] - 25s 795us/step - loss: 0.6295\n",
      "Epoch 491/500\n",
      "31412/31412 [==============================] - 25s 798us/step - loss: 0.6283\n",
      "Epoch 492/500\n",
      "31412/31412 [==============================] - 25s 786us/step - loss: 0.6367\n",
      "Epoch 493/500\n",
      "31412/31412 [==============================] - 25s 793us/step - loss: 0.6257\n",
      "Epoch 494/500\n",
      "31412/31412 [==============================] - 25s 792us/step - loss: 0.6348\n",
      "Epoch 495/500\n",
      "31412/31412 [==============================] - 25s 788us/step - loss: 0.6313\n",
      "Epoch 496/500\n",
      "31412/31412 [==============================] - 25s 788us/step - loss: 0.6315\n",
      "Epoch 497/500\n",
      "31412/31412 [==============================] - 25s 796us/step - loss: 0.6350\n",
      "Epoch 498/500\n",
      "31412/31412 [==============================] - 25s 805us/step - loss: 0.6262\n",
      "Epoch 499/500\n",
      "31412/31412 [==============================] - 25s 798us/step - loss: 0.6159\n",
      "Epoch 500/500\n",
      "31412/31412 [==============================] - 25s 791us/step - loss: 0.6247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x20efdf97d68>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=500, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Write the beginning of your poem, the Shakespeare machine will complete it. Your input is:  Shall I compare thee to a Summers day?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Here is your poem: \n",
      "\n",
      "Shall I compare thee to a Summers day?\n",
      "hask clool whathy carour wordh'st which dethright word,\n",
      "hadd my for yee for the eyes and lif,\n",
      "the manker place up the larker lins, or thene.\n",
      "thine my nother i an their eyes be gabkelss glend,\n",
      "thee withing mister bord frow the beauty,\n",
      "whe houdss for the woms of hovsed angirs,\n",
      "i celper thy dear were which thee his dogund,\n",
      "far their live fal forguly epiny your lie,\n",
      "your do makest theught that upen a"
     ]
    }
   ],
   "source": [
    "# Run this cell to try with different inputs without having to re-train the model \n",
    "generate_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_shakespeare.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN-Shakespeare model is very similar to the one you have built for dinosaur names. The only major differences are:\n",
    "- LSTMs instead of the basic RNN to capture longer-range dependencies\n",
    "- The model is a deeper, stacked LSTM model (2 layer)\n",
    "- Using Keras instead of python to simplify the code \n",
    "\n",
    "If you want to learn more, you can also check out the Keras Team's text generation implementation on GitHub: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py.\n",
    "\n",
    "Congratulations on finishing this notebook! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "- This exercise took inspiration from Andrej Karpathy's implementation: https://gist.github.com/karpathy/d4dee566867f8291f086. To learn more about text generation, also check out Karpathy's [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "- For the Shakespearian poem generator, our implementation was based on the implementation of an LSTM text generator by the Keras team: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "1dYg0",
   "launcher_item_id": "MLhxP"
  },
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
